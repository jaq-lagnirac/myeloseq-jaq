# Justin Caringal
# Compares whether the coverage in the MyeloseqHD JSON files
# matches the coverage in the BED files and generates a
# comparison table


import os
import sys
import argparse
import logging
import json
import pandas as pd
from intervaltree import Interval, IntervalTree

JSON_SUFFIX = '.report.json'
BED_SUFFIX = '.qc-coverage-region-1_full_res.bed'

SEP = '\t'

ROUNDING_DECIMALS = 3

SCRIPT_PATH = os.path.abspath(__file__)
FORMAT = '[%(asctime)s] %(levelname)s %(message)s'
l = logging.getLogger()
lh = logging.StreamHandler()
lh.setFormatter(logging.Formatter(FORMAT))
l.addHandler(lh)
l.setLevel(logging.INFO)
debug = l.debug; info = l.info; warning = l.warning; error = l.error

DESCRIPTION = '''

Takes directory of directories (generated by generate_directories.py)
and generates a comparison table to compare the coverage of the
requisite JSON and BED files

'''

EPILOG = '''

Outputs statistics in .err file

'''

class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter,
    argparse.RawDescriptionHelpFormatter):
  pass
parser = argparse.ArgumentParser(description=DESCRIPTION, epilog=EPILOG,
  formatter_class=CustomFormatter)

parser.add_argument('directory',
                    help='main directory for comparison')
parser.add_argument('-v', '--verbose',
                    action='store_true',
                    help='Set logging level to DEBUG')

args = parser.parse_args()

if args.verbose:
  l.setLevel(logging.DEBUG)



def process_json_entry(pos, ref, alt):
  # SNV
  if (len(ref) == 1) and (len(ref) == 1):
    start = pos
    end = pos
  # DELETION
  elif (len(ref) > 1) and (len(alt) == 1):
    start = pos + 1
    end = pos + len(ref) - 1
  # INSERTION
  elif (len(ref) == 1) and (len(alt) > 1):
    start = pos
    end = pos + 1
  else:
    start = pos
    end = start + len(alt) - 1

  return start, end



debug('%s begin', SCRIPT_PATH)

# Initialize counters
total_comparisons = 0
json_greater = 0
bed_greater = 0
coverage_equal = 0

# Initialize totals (for averages)
json_total = 0
bed_total = 0
vs_total = 0

# Initialize dictionary and fields (future dataframe)
fields = ['directory name',
          'chrom',
          'start',
          'end',
          'ref',
          'alt',
          'json coverage',
          'bed coverage',
          'json vs bed coverage']
table_dict = {}
for field in fields:
  table_dict[field] = []

# Run through main directory
for directory_name in os.listdir(args.directory):
  # Genereate file names
  json_name = f'{directory_name}{JSON_SUFFIX}'
  bed_name = f'{directory_name}{BED_SUFFIX}'

  # Generate file paths
  json_path = os.path.join(args.directory,
                           directory_name,
                           json_name)
  bed_path = os.path.join(args.directory,
                          directory_name,
                          bed_name)


  # Process JSON file
  
  with open(json_path) as jp:
    json_file = json.loads(jp.read())
  
  try:
    tier13_columns = json_file['VARIANTS']['TIER1-3']['columns']
    tier13_data = json_file['VARIANTS']['TIER1-3']['data']
    info(f'Accessing file: {json_path}')
  except:
    error(f'Tier 1-3 columns do not exist: {json_path}')
    continue
  
  
  # Process BED file

  df = pd.read_csv(bed_path,
                   sep = SEP,
                   names = ['chrom',
                            'start',
                            'end',
                            'coverage'])
  
  # Create interval tree from pandas dataframe
  bed_tree = IntervalTree()
  for index, row in df.iterrows():
    bed_tree[row['start'] : row['end']] = row['coverage']


  # Comparing files

  for entry in tier13_data:
    
    # JSON data extraction
    chrom = entry[tier13_columns.index('chrom')]
    pos = int(entry[tier13_columns.index('pos')])
    ref = entry[tier13_columns.index('ref')]
    alt = entry[tier13_columns.index('alt')]
    json_cov = entry[tier13_columns.index('coverage')]
    
    start, end = process_json_entry(pos, ref, alt)

    # -1 accounting for 1-based to 0-based conversion
    # BED files are 0-based half-open [ )
    # JSON files are 1-based
    start -= 1

    # Comparison code
    interval_set = bed_tree[start : end] # overlaps
    #bed_cov = bed_tree.envelop(start, end)
    for interval in interval_set:
      # extracts data from interval, compares
      bed_cov = interval.data
      vs_cov = json_cov - bed_cov

      # sets up data list to append to table dict 
      data = [directory_name,
              chrom,
              start,
              end,
              ref,
              alt,
              json_cov,
              bed_cov,
              vs_cov]
      # appends data to table_dict
      for index, field in enumerate(fields):
        table_dict[field].append(data[index])

      # Output checkpoints and increments counters
      total_comparisons += 1
      if json_cov > bed_cov:
        json_greater += 1
        info('JSON has greater coverage than BED')
      elif bed_cov > json_cov:
        bed_greater += 1
        info('BED has greater coverage than JSON')
      elif bed_cov == json_cov:
        coverage_equal += 1
        info('Coverage is equal')
      else:
        info('Unknown comparison')
      
      # Increments totals for averages
      json_total += json_cov
      bed_total += bed_cov
      vs_total += vs_cov

# Converts dict to tsv
output_df = pd.DataFrame.from_dict(table_dict)
output_df.to_csv(sys.stdout, sep=SEP, index=None)

# Calculates averages
json_avg = round(json_total / total_comparisons, ROUNDING_DECIMALS)
bed_avg = round(bed_total / total_comparisons, ROUNDING_DECIMALS)
vs_avg = round(vs_total / total_comparisons, ROUNDING_DECIMALS)

# Output statistics

info(f'Total Comparisons: {total_comparisons}')
info(f'Cases where JSON coverage was greater: {json_greater}')
info(f'Cases where BED coverage was greater: {bed_greater}')
info(f'Cases where coverage was equal: {coverage_equal}')
info(f'JSON average coverage: {json_avg}')
info(f'BED average coverage: {bed_avg}')
info(f'Average coverage comparison: {vs_avg}')

debug('%s end', SCRIPT_PATH)