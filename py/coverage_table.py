# Justin Caringal
# Compares whether the coverage in the MyeloseqHD JSON files
# matches the coverage in the BED files and generates a
# comparison table


import os
import sys
import argparse
import logging
import json
import pandas as pd
from intervaltree import IntervalTree

JSON_SUFFIX = '.report.json'
BED_SUFFIX = '.qc-coverage-region-1_full_res.bed'

SEP = '\t'

ROUNDING_DECIMALS = 3
REL_DIFF_ROUND = 5

SCRIPT_PATH = os.path.abspath(__file__)
FORMAT = '[%(asctime)s] %(levelname)s %(message)s'
l = logging.getLogger()
lh = logging.StreamHandler()
lh.setFormatter(logging.Formatter(FORMAT))
l.addHandler(lh)
l.setLevel(logging.INFO)
debug = l.debug; info = l.info; warning = l.warning; error = l.error

DESCRIPTION = '''

Takes directory of directories (generated by generate_directories.py)

and generates a comparison table to compare the coverage of the

requisite JSON and BED files.

'''

EPILOG = '''

Outputs statistics in .err file.

This is an extension of json_bed_comparison.py.

'''

class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter,
    argparse.RawDescriptionHelpFormatter):
  pass
parser = argparse.ArgumentParser(description=DESCRIPTION, epilog=EPILOG,
  formatter_class=CustomFormatter)

parser.add_argument('directory',
                    help='main directory for comparison')
parser.add_argument('-t', '--variant-type',
                    nargs='+',
                    default='TIER1-3',
                    help='Variant type to be extracted')
parser.add_argument('-v', '--verbose',
                    action='store_true',
                    help='Set logging level to DEBUG')

args = parser.parse_args()

if args.verbose:
  l.setLevel(logging.DEBUG)



# Initialize dictionary and fields
fields = ['directory name',
          'variant type',
          'filter',
          'chrom',
          'start',
          'end',
          'ref',
          'alt',
          'gene',
          'transcript',
          'json coverage',
          'bed coverage',
          'json vs bed coverage',
          'relative difference']
table_dict = {} # will be turned into a dataframe
for field in fields:
  table_dict[field] = []



def list_to_dict(keys, values):
  # takes two lists, converts into dict

  # error handling
  if len(keys) != len(values):
    error('Keys and values mismatch.')
    error(f'Key length: {len(keys)}, Value length: {len(values)}')
    sys.exit(1)
  
  # bulk of code
  return_dict = {}
  for index, key in enumerate(keys):
    return_dict[key] = values[index]
  
  return return_dict




def process_json_entry(pos, ref, alt):
  # SNV
  if (len(ref) == 1) and (len(ref) == 1):
    start = pos
    end = pos
  # DELETION
  elif (len(ref) > 1) and (len(alt) == 1):
    start = pos + 1
    end = pos + len(ref) - 1
  # INSERTION
  elif (len(ref) == 1) and (len(alt) > 1):
    start = pos
    end = pos + 1
  else:
    start = pos
    end = start + len(alt) - 1

  return start, end



def intersection(list1, list2):
  # set theory intersection
  list3 = [value for value in list1 if value in list2]
  return list3



def check_duplicates(data_dict, table_dict):
  dict_of_index_lists = {}

  # fields to be compared
  # NOTE: removed .index and tying it to fields in order
  # to make it more universal and open to change
  shortened_fields = ['chrom',
                      'start',
                      'end',
                      'ref',
                      'alt']
  
  # generates dict of index lists
  for field in shortened_fields:
    comparison_value = data_dict[field] # string or int
    table_list = table_dict[field] # list

    # initializes list of indexes of all occurences in table_list
    index_list = []
    for index, table_list_value in enumerate(table_list):
      if comparison_value == table_list_value:
        index_list.append(index)
    
    # cleans index_list, adds to dict
    index_list = list(set(index_list)) # removes duplicate indexes
    index_list.sort()
    dict_of_index_lists[field] = index_list
  
  # compares lists to see if there is a common index across all fields
  # finds intersection of all of the index comparison lists, leaving
  # only one index left
  comparison_list = dict_of_index_lists['chrom']
  for field in shortened_fields[1 : ]:
    comparison_list = intersection(comparison_list,
                                   dict_of_index_lists[field])
  
  # More than one duplicate detected
  # Should never be triggered as duplicates are deleted as soon
  # as two of the same are detected, but included to handle
  # any possible errors
  if len(comparison_list) > 1:
    error('Duplicate located in more than one location.')
    sys.exit(1)

  # No discrepancies detected
  if len(comparison_list) == 0:
    return False # unique data, skips over duplicate handling
  
  # returns single int index
  # Python sees this as boolean True
  return comparison_list[0]



debug('%s begin', SCRIPT_PATH)

# Initialize counters
total_comparisons = 0
duplicate_count = 0
json_greater = 0
bed_greater = 0
coverage_equal = 0
error_count = 0
directory_count = 0

# Initialize totals (for averages)
json_total = 0
bed_total = 0
vs_total = 0
rel_diff_total = 0

# Run through main directory
for directory_name in os.listdir(args.directory):
  directory_count += 1

  # Genereate file names
  json_name = f'{directory_name}{JSON_SUFFIX}'
  bed_name = f'{directory_name}{BED_SUFFIX}'

  # Generate file paths
  json_path = os.path.join(args.directory,
                           directory_name,
                           json_name)
  bed_path = os.path.join(args.directory,
                          directory_name,
                          bed_name)


  # Process JSON file
  
  with open(json_path) as jp:
    json_file = json.loads(jp.read())
  
  # if the file doesn't contain column "TIER1-3" (or other variant)
  # ignore the rest of the loop, continue to the next iteration,
  # and read in the next file
  try:
    variant_columns = json_file['VARIANTS'][args.variant_type]['columns']
    variant_data = json_file['VARIANTS'][args.variant_type]['data']
    info(f'Accessing file for {args.variant_type}: {json_name}')
  except:
    error(f'{args.variant_type} columns do not exist: {json_name}')
    error_count += 1
    continue
  
  
  # Process BED file

  info(f'Accessing file: {bed_name}')
  bed_df = pd.read_csv(bed_path,
                       sep = SEP,
                       names = ['chrom',
                                'start',
                                'end',
                                'coverage'])
  
  # Create interval tree from pandas dataframe
  bed_tree = IntervalTree()
  for index, row in bed_df.iterrows():
    bed_tree[row['start'] : row['end']] = row['coverage']


  # Comparing files

  for entry in variant_data:
    
    # JSON data extraction
    filter = entry[variant_columns.index('filter')]
    chrom = entry[variant_columns.index('chrom')]
    pos = int(entry[variant_columns.index('pos')])
    ref = entry[variant_columns.index('ref')]
    alt = entry[variant_columns.index('alt')]
    gene = entry[variant_columns.index('gene')]
    transcript = entry[variant_columns.index('transcript')]
    json_cov = entry[variant_columns.index('coverage')]
    
    # creates start and end using maf string-like processing
    start, end = process_json_entry(pos, ref, alt)

    # NOTE: -1 accounting for 1-based to 0-based conversion
    # BED files are 0-based half-open [ )
    # JSON files are 1-based
    start -= 1


    # Comparison code

    interval_set = bed_tree[start : end] # overlaps

    for interval in interval_set:

      # increment comparison counter
      total_comparisons += 1

      # extracts data from interval, compares
      bed_cov = interval.data
      vs_cov = json_cov - bed_cov
      rel_diff = round(vs_cov / bed_cov, REL_DIFF_ROUND)

      # sets up data dict to compare and append to table dict
      # see list "fields" for heading list
      data_list = [directory_name,
                   args.variant_type,
                   filter,
                   chrom,
                   start,
                   end,
                   ref,
                   alt,
                   gene,
                   transcript,
                   json_cov,
                   bed_cov,
                   vs_cov,
                   rel_diff] 
      data_dict = list_to_dict(fields, data_list)
      
      # returns index of duplicate, false otherwise
      duplicate_index = check_duplicates(data_dict, table_dict)
      
      # only runs if duplicate present
      if duplicate_index:
        # increments duplicate coverage counter
        duplicate_count += 1

        # if new absolute vs coverage is greater than coverage in table_dict
        # greater difference, i.e. more noticeable
        if abs(vs_cov) > abs(table_dict['json vs bed coverage'][duplicate_index]):

          debug('Higher BED coverage found. Updating table and averages.')
          
          # then update totals (for averages)
          bed_total += bed_cov - table_dict['bed coverage'][duplicate_index]
          vs_total += vs_cov - table_dict['json vs bed coverage'][duplicate_index]
          rel_diff_total += rel_diff - table_dict['relative difference'][duplicate_index]

          # and change values to reflect new change
          table_dict['bed coverage'][duplicate_index] = bed_cov
          table_dict['json vs bed coverage'][duplicate_index] = vs_cov
          table_dict['relative difference'][duplicate_index] = rel_diff

          # NOTE: Duplicates should not affect count,
          # as none observed have changed signs

        else:
          debug('Lower or equal absolute value VS coverage found.')
        
        continue
      

      ### following only runs if NO duplicates are found

      # appends data to table_dict field by field
      for field in fields:
        table_dict[field].append(data_dict[field])
      
      # Output checkpoints and increments counters
      # NOTE: Duplicates should not affect count, as none observed have changed signs
      if json_cov > bed_cov:
        json_greater += 1
        debug('JSON has greater coverage than BED')

      elif bed_cov > json_cov:
        bed_greater += 1
        debug('BED has greater coverage than JSON')

      elif bed_cov == json_cov:
        coverage_equal += 1
        debug('Coverage is equal')

      else: # should never trigger, but will stop as a safety measure
        error('Unknown comparison')
        sys.exit(1)
      
      # Increments totals for averages
      json_total += json_cov
      bed_total += bed_cov
      vs_total += vs_cov
      rel_diff_total += rel_diff

# Converts dict to tsv
output_df = pd.DataFrame.from_dict(table_dict)
output_df = output_df.sort_values(by='json vs bed coverage', ascending=False)
output_df.to_csv(sys.stdout, sep=SEP, index=None)

total_no_duplicates = total_comparisons - duplicate_count

# Calculates percent (does NOT multiply by 100, leaves as decimal)
json_percent = round(json_greater / total_no_duplicates, ROUNDING_DECIMALS)
bed_percent = round(bed_greater / total_no_duplicates, ROUNDING_DECIMALS)
equal_percent = round(coverage_equal / total_no_duplicates, ROUNDING_DECIMALS)

# Calculates averages
json_avg = round(json_total / total_no_duplicates, ROUNDING_DECIMALS)
bed_avg = round(bed_total / total_no_duplicates, ROUNDING_DECIMALS)
vs_avg = round(vs_total / total_no_duplicates, ROUNDING_DECIMALS)
rel_diff_avg = round(rel_diff_total / total_no_duplicates, REL_DIFF_ROUND)
error_percent = round(error_count / directory_count, ROUNDING_DECIMALS)


# Output statistics

info('----------DIRECTORTY STATISTICS----------')
info(f'Total subdirectories accessed: {directory_count}')
info(f'JSON files without correct column (error count): {error_count}')
info(f'Subirectory error percent: {error_percent}')

info('----------COMPARISON STATISTICS----------')
info(f'Total Comparisons: {total_comparisons}')
info(f'Duplicate coverage count: {duplicate_count}')
info(f'Total Comparisons, excluding duplicates: {total_no_duplicates}')

info('----------STATISTICS EXCLUDING DUPLICATES----------')
info(f'Cases where JSON coverage was greater: {json_percent} ({json_greater})')
info(f'Cases where BED coverage was greater: {bed_percent} ({bed_greater})')
info(f'Cases where coverage was equal: {equal_percent} ({coverage_equal})')
info(f'JSON average coverage: {json_avg}')
info(f'BED average coverage: {bed_avg}')
info(f'Average coverage comparison: {vs_avg}')
info(f'Average relative difference: {rel_diff_avg}')

debug('NOTE: Average coverage calculated by (JSON coverage) - (BED coverage) for each interval')
debug('NOTE: +(pos) indicates higher average JSON coverage, -(neg) indicated higher average BED coverage')
debug('NOTE: Relative Difference calculated by ((JSON coverage - BED coverage) / BED coverage)')

debug('%s end', SCRIPT_PATH)