# Justin Caringal
# Compares whether the coverage in the MyeloseqHD JSON files
# matches the coverage in the BED files and generates a
# comparison table


import os
import sys
import argparse
import logging
import json
import pandas as pd
from intervaltree import Interval, IntervalTree

JSON_SUFFIX = '.report.json'
BED_SUFFIX = '.qc-coverage-region-1_full_res.bed'

SEP = '\t'

ROUNDING_DECIMALS = 3

SCRIPT_PATH = os.path.abspath(__file__)
FORMAT = '[%(asctime)s] %(levelname)s %(message)s'
l = logging.getLogger()
lh = logging.StreamHandler()
lh.setFormatter(logging.Formatter(FORMAT))
l.addHandler(lh)
l.setLevel(logging.INFO)
debug = l.debug; info = l.info; warning = l.warning; error = l.error

DESCRIPTION = '''

Takes directory of directories (generated by generate_directories.py)

and generates a comparison table to compare the coverage of the

requisite JSON and BED files.

'''

EPILOG = '''

Outputs statistics in .err file.

This is an extension of json_bed_comparison.py.

'''

class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter,
    argparse.RawDescriptionHelpFormatter):
  pass
parser = argparse.ArgumentParser(description=DESCRIPTION, epilog=EPILOG,
  formatter_class=CustomFormatter)

parser.add_argument('directory',
                    help='main directory for comparison')
parser.add_argument('-v', '--verbose',
                    action='store_true',
                    help='Set logging level to DEBUG')

args = parser.parse_args()

if args.verbose:
  l.setLevel(logging.DEBUG)



# Initialize dictionary and fields (future dataframe)
fields = ['directory name',
          'chrom',
          'start',
          'end',
          'ref',
          'alt',
          'gene',
          'transcript',
          'json coverage',
          'bed coverage',
          'json vs bed coverage']
table_dict = {}
for field in fields:
  table_dict[field] = []



def list_to_dict(keys, values):
  if len(keys) != len(values):
    error('Keys and values mismatch.')
    sys.exit(1)
  
  return_dict = {}
  for index, key in enumerate(keys):
    return_dict[key] = values[index]
  
  return return_dict




def process_json_entry(pos, ref, alt):
  # SNV
  if (len(ref) == 1) and (len(ref) == 1):
    start = pos
    end = pos
  # DELETION
  elif (len(ref) > 1) and (len(alt) == 1):
    start = pos + 1
    end = pos + len(ref) - 1
  # INSERTION
  elif (len(ref) == 1) and (len(alt) > 1):
    start = pos
    end = pos + 1
  else:
    start = pos
    end = start + len(alt) - 1

  return start, end



def intersection(list1, list2):
  list3 = [value for value in list1 if value in list2]
  return list3



def check_duplicates(data_dict, table_dict):
  #return False
  dict_of_index_lists = {}

  shortened_fields = fields[fields.index('chrom') : fields.index('alt')]
  
  # generates dict of index lists
  for field in shortened_fields:
    comparison_value = data_dict[field] # string or int
    table_list = table_dict[field] # list

    # initializes list of indexes of all occurences in table_list
    index_list = []
    for index, table_list_value in enumerate(table_list):
      if comparison_value == table_list_value:
        index_list.append(index)
    
    # cleans index_list, adds to dict
    index_list = list(set(index_list)) # removes duplicate indexes
    index_list.sort()
    dict_of_index_lists[field] = index_list
  
  # compares lists to see if there is a common index across all fields
  comparison_list = dict_of_index_lists['chrom']
  for field in shortened_fields[1 : ]:
    comparison_list = intersection(comparison_list,
                                   dict_of_index_lists[field])
  
  # More than one duplicate detected
  if len(comparison_list) > 1:
    error('Duplicate located in more than one location.')
    sys.exit(1)

  # No discrepancies detected
  if len(comparison_list) == 0:
    return False # unique data, skips over duplicate handling
  return comparison_list[0] # returns single int index



debug('%s begin', SCRIPT_PATH)

# Initialize counters
total_comparisons = 0
duplicate_count = 0
json_greater = 0
bed_greater = 0
coverage_equal = 0
error_count = 0
directory_count = 0

# Initialize totals (for averages)
json_total = 0
bed_total = 0
vs_total = 0 

# Run through main directory
for directory_name in os.listdir(args.directory):
  directory_count += 1

  # Genereate file names
  json_name = f'{directory_name}{JSON_SUFFIX}'
  bed_name = f'{directory_name}{BED_SUFFIX}'

  # Generate file paths
  json_path = os.path.join(args.directory,
                           directory_name,
                           json_name)
  bed_path = os.path.join(args.directory,
                          directory_name,
                          bed_name)


  # Process JSON file
  
  with open(json_path) as jp:
    json_file = json.loads(jp.read())
  
  try:
    tier13_columns = json_file['VARIANTS']['TIER1-3']['columns']
    tier13_data = json_file['VARIANTS']['TIER1-3']['data']
    info(f'Accessing file: {json_path}')
  except:
    error(f'Tier 1-3 columns do not exist: {json_path}')
    error_count += 1
    continue
  
  
  # Process BED file

  df = pd.read_csv(bed_path,
                   sep = SEP,
                   names = ['chrom',
                            'start',
                            'end',
                            'coverage'])
  
  # Create interval tree from pandas dataframe
  bed_tree = IntervalTree()
  for index, row in df.iterrows():
    bed_tree[row['start'] : row['end']] = row['coverage']


  # Comparing files

  for entry in tier13_data:
    
    # JSON data extraction
    chrom = entry[tier13_columns.index('chrom')]
    pos = int(entry[tier13_columns.index('pos')])
    ref = entry[tier13_columns.index('ref')]
    alt = entry[tier13_columns.index('alt')]
    gene = entry[tier13_columns.index('gene')]
    transcript = entry[tier13_columns.index('transcript')]
    json_cov = entry[tier13_columns.index('coverage')]
    
    start, end = process_json_entry(pos, ref, alt)

    # -1 accounting for 1-based to 0-based conversion
    # BED files are 0-based half-open [ )
    # JSON files are 1-based
    start -= 1

    # Comparison code
    interval_set = bed_tree[start : end] # overlaps
    #bed_cov = bed_tree.envelop(start, end)
    for interval in interval_set:

      # increment comparison counter
      total_comparisons += 1

      # extracts data from interval, compares
      bed_cov = interval.data
      vs_cov = json_cov - bed_cov

      # sets up data dict to compare and append to table dict
      # see list "fields" for heading list
      data_list = [directory_name,
                   chrom,
                   start,
                   end,
                   ref,
                   alt,
                   gene,
                   transcript,
                   json_cov,
                   bed_cov,
                   vs_cov] 
      data_dict = list_to_dict(fields, data_list)
      
      # returns index of duplicate, false otherwise
      duplicate_index = check_duplicates(data_dict, table_dict)
      
      # only runs if duplicate present
      if duplicate_index:
        # increments duplicate coverage counter
        duplicate_count += 1

        # if new bed coverage is greater than coverage in table_dict
        if bed_cov > table_dict['bed coverage'][duplicate_index]:

          debug('Higher BED coverage found. Updating table and averages.')
          
          # then update totals (for averages)
          bed_total += bed_cov - table_dict['bed coverage'][duplicate_index]
          vs_total += vs_cov - table_dict['json vs bed coverage'][duplicate_index]

          # and change both bed_cov and vs_cov to reflect new change
          table_dict['bed coverage'][duplicate_index] = bed_cov
          table_dict['json vs bed coverage'][duplicate_index] = vs_cov

        else:
          debug('Lower or equal BED coverage found.')
        
        continue
      
      ### following only runs if NO duplicates are found

      # appends data to table_dict
      for field in fields:
        table_dict[field].append(data_dict[field])
      
      # Output checkpoints and increments counters
      # Duplicates should not affect count, as none observed have changed signs
      if json_cov > bed_cov:
        json_greater += 1
        debug('JSON has greater coverage than BED')
      elif bed_cov > json_cov:
        bed_greater += 1
        debug('BED has greater coverage than JSON')
      elif bed_cov == json_cov:
        coverage_equal += 1
        debug('Coverage is equal')
      else: # should never trigger, but will stop as a safety measure
        error('Unknown comparison')
        sys.exit(1)
      
      # Increments totals for averages
      json_total += json_cov
      bed_total += bed_cov
      vs_total += vs_cov

# Converts dict to tsv
output_df = pd.DataFrame.from_dict(table_dict)
output_df.to_csv(sys.stdout, sep=SEP, index=None)

total_no_duplicates = total_comparisons - duplicate_count

# Calculates percent
json_percent = round(json_greater / total_no_duplicates, ROUNDING_DECIMALS)
bed_percent = round(bed_greater / total_no_duplicates, ROUNDING_DECIMALS)
equal_percent = round(coverage_equal / total_no_duplicates, ROUNDING_DECIMALS)

# Calculates averages
json_avg = round(json_total / total_no_duplicates, ROUNDING_DECIMALS)
bed_avg = round(bed_total / total_no_duplicates, ROUNDING_DECIMALS)
vs_avg = round(vs_total / total_no_duplicates, ROUNDING_DECIMALS)
error_percent = round(error_count / directory_count, ROUNDING_DECIMALS)

# Output statistics

info('----------DIRECTORTY STATISTICS----------')
info(f'Total subdirectories accessed: {directory_count}')
info(f'JSON files without column \"TIER1-3\" (error count): {error_count}')
info(f'Subirectory error percent: {error_percent}')

info('----------COMPARISON STATISTICS----------')
info(f'Total Comparisons: {total_comparisons}')
info(f'Duplicate coverage count: {duplicate_count}')
info(f'Total Comparisons, excluding duplicates: {total_no_duplicates}')
info(f'Cases where JSON coverage was greater: {json_percent} ({json_greater})')
info(f'Cases where BED coverage was greater: {bed_percent} ({bed_greater})')
info(f'Cases where coverage was equal: {equal_percent} ({coverage_equal})')
info(f'JSON average coverage: {json_avg}')
info(f'BED average coverage: {bed_avg}')
info(f'Average coverage comparison: {vs_avg}')
info('NOTE: Average coverage calculated by (JSON coverage) - (BED coverage) for each interval')
info('NOTE: +(pos) indicates higher average JSON coverage, -(neg) indicated higher average BED coverage')

debug('%s end', SCRIPT_PATH)